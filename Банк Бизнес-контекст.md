# Банк

### Команда:
4 бека
1 фронт
1  devops
1 QA
1 teamlead

---

## Контекст

Разработка бекенда внутреннего сервиса компании по администрированию всех активностей сотрудников организации. Различные командировки, отпуска, больничные. Сервис позволяет руководителям подразделений мониторить табель учета рабочего времени своих подчиненных. Так же отдел бухгалтерии использует этот сервис непосредственно для подсчета рабочих часов сотрудника и на основе его считают оклад от выработанных часов.

---

## Почему уходишь с текущего места работы?

Я работаю по срочному контракту на позиции разработчика, замещая коллегу, который находился в отпуске по уходу за ребёнком. Через месяц он возвращается, и моя позиция заканчивается. Работодатель предложил перейти в другой отдел, но на проект, который сейчас находится в режиме поддержки, и мне было бы интереснее развиваться в более активном направлении. Поэтому я начал поиск новой позиции.

---

## Предыстория (только если спросят что-то, сам не рассказывай)

Попал на проект чисто случайно, проходил собеседование на junior-devops(была интересна эта тема) и в ходе собеседования рассказал о своем опыте разработчика. 

Через несколько дней мне позвонили и предложили заключить срочный контракт на позицию python-разработчика, я принял контракт, тк был опыт уже с этим стеком.

Проект уже выполнял необходимый функционал, тк его делали для отдела разработки, но старшие руководители увидели перспективы в нем, тк подобный сервис поставлялся иностранной компанией и не было уверенности в его дальнейшей поддержке на территории РФ, поэтому было принято решение увеличить штат сотрудников на 2 разработчика(1 для расширения, 1 на замену декретницы) 

На тот момент сервис работал стабильно, но оказалось, что нагрузка увеличится в несколько раз(штат it был примерно 2к, а общий штат 25к). Несмотря на рост нагрузки решили не переходить на kafka, изначально сразу был rabbit. 

Технологии были выбраны исходя из компетенций разработчиков (оба работали с python + fastapi). 

Когда я пришел стояла задача расширить функционал сервисов, добавить множество ролей авторизации, внедрить большое кол-во нотификаций, возникла потребность разгрузить кадровых сотрудников(поэтому началась работа с внедрением ИИ-агентов), написание тестов, некоторое изменение структуры БД.

---

## Сервисы:

User - управление сотрудниками
Managment service - создание заявок на активности
calendar service - отображение графиков бл, отпусков и тд
Analytics service - формирование отчетов по раб. времени
notification - уведомление сотрудников об изменениях и добавлениях акт.
authorization - управление доступом к данным в зависимости от роли

>Взаимодействие между сервисами было через RabbitMQ + Celery
>Так же был настроен redis, в качестве кеша к часто запрашиваемым данным

---

## Моя роль в проекте включала в себя:

Основные сервисы в моей зоне ответственности:
1. notification
2. Analytics service
3. Managment service
Однако мы все должны были подтвердить коммиты в прод из сервисов друг друга, чтобы постоянно быть в контексте(на случай отсутствия кого-то из коллег)

---

## Что сложного делал за последнее время?

Последние 3 месяцев занимался внедрением ИИ-агента, который оптимизирует первый этап работы с заявлениями на увольнение сотрудников. Получает скан заявление на увольнение, проверяет корректность заполнения полей, на основе анализа LLM формирует структурированный вывод:
Если найдена ошибка - отправляет автоматический ответ сотруднику с перечнем необходимых изменений

при успешной проверке - создает в менеджерском сервисе заявку на увольнения сотрудника

работа еще не внедрена окончательно, однако ЛМ уже справляется в 8 из 10 случаях, сейчас идет работа над исправлениями багов, связанных с галлюцинацими ИИ

(Такой подход позволит снизить нагрузку на HR-отдел за счёт исключения рутинной проверки первичных данных, ускорит начало обработки заявок и минимизировать количество ошибок на начальном этапе.)

---

## Факап

На одном из этапов разработки HR-микросервиса я занимался реализацией функционала уведомлений. В частности, нужно было отправлять уведомление сотруднику, когда подтвержденные заявки приходили ему в почту.

Моя задача состояла в том, чтобы при одобрении заявки через систему запускалась асинхронная задача (Celery), которая на почту сотрудника отправляла уведомление о том, что заявка согласована

Я реализовал интеграцию с Celery, написал обработчик события “заявка одобрена”, сделал шаблон уведомления и вызов почтового сервиса.

В тестах я проверил не все заявки. Некорректно отрабатывала заявка на отпуск, письмо приходило не конкретному сотруднику, а всем сотрудникам из его отдела. Причина ошибки — невнимательное копирование кода из другой части системы, где действительно требовалось оповещать весь отдел

**Что я сделал:**

- Проверил логи Celery-задачи.
- Нашёл место, где формировался список получателей.
- Исправил код: заменил получателей на конкретного сотрудника отдела
- Добавил проверку в unit-тесты, чтобы в будущем подобные ошибки ловились автоматически.

Это помогло мне быть более внимательным при использовании общего кода и сторонних функций.

---

## Почему Rabbit а не kafka

**Поддержка Celery "из коробки":**  
У нас активно используется Celery для асинхронных задач — отправка email, генерация отчетов. RabbitMQ имеет отличную интеграцию с Celery, что упрощает разработку и эксплуатацию, включая работу с тасками, retries/

**Меньше сложностей с операционной частью:**  
Для нашей команды из 8 человек (включая DevOps) было важно выбрать более легковесное и простое в обслуживании решение. Kafka требует больше ресурсов и усилий по настройке, мониторингу и масштабированию. Пока нет потребности в миллионах сообщений в секунду или долгосрочном хранении логов — RabbitMQ полностью перекрывает наши потребности.

**Проект не требует высокого уровня горизонтального масштабирования очередей:**  
Поскольку это внутренний сервис, нагрузка предсказуемая и не экстремальная, RabbitMQ показывает себя отлично. Мы не стоим перед необходимостью обрабатывать миллионы событий в реальном времени

---

## RabbitMQ

### Подключение: (сделаю после просмотра курса по rabbit)
### Настраивал ли сам exchange?

> Сам exchange настраивал DevOps, потому что это часть более глубокой настройки RabbitMQ. Но я хорошо понимаю, как он работает.
> 
> У нас был один **default exchange** , который использовала Celery по умолчанию. Мы не задействовали кастомные типы exchange (fanout, topic и т.п.), потому что нам не требовалась сложная маршрутизация.
> 
> В рамках моих задач я работал скорее с consumer-частью: читал из нужной очереди, обрабатывал данные, управлял retry и логировал ошибки.

---

## Метрики

Мы использовали Prometheus и Grafana для мониторинга сервисов. Через middleware `starlette_exporter` собирали HTTP-метрики. Также использовали PostgreSQL Exporter для системных метрик БД. Это позволило нам оперативно реагировать на проблемы с производительностью, снижало время диагностики и повышало надёжность сервиса.

---

## Процессы

---

## Kuber

В нашей команде деплой системы осуществлялся через автоматизированный CI/CD-процесс, который был настроен совместно с нашим DevOps-инженером. Мы использовали GitLab для автоматизации сборки, тестирования и публикации Docker-образов. После успешного прохождения всех этапов CI/CD, образы отправлялись в private Docker-репозиторий. Процесс деплоя самой системы управлялся DevOps-инженером, который отвечал за конфигурацию и развертывание приложений в Kubernetes. Однако я активно участвовал в подготовке Dockerfile'ов

Я знаком с основными компонентами Kubernetes и понимаю их роль в управлении микросервисной архитектурой:

- **Pods** : базовая единица развертывания.
- **Deployments** : обеспечивают управление Pod'ами и гарантируют, что всегда работает заданное количество экземпляров.
- **Services** : предоставляют стабильный способ доступа к группе Pod'ов.
- **ConfigMaps и Secrets** : хранят конфигурационные данные и секреты.
- **Namespace** : помогают организовать ресурсы по проектам или командам.

В основном моя задача заключалась в подготовке необходимых ресурсов для деплоя:

- Создавал Dockerfile'ы для наших сервисов, оптимизируя размер образов и скорость запуска.

Для просмотра логов контейнеров мы использовали следующие команды Kubernetes CLI (`kubectl`):
```bash
kubectl logs <pod-name> -n <namespace>
```


**Как создавал новые сервисы?**

Создание новых сервисов происходило следующим образом:

1. **Разработка сервиса** : Я писал код нового сервиса на Python, используя фреймворки типа Flask или FastAPI, и тестировал его локально.
2. **Контейнеризация** : Создавал Dockerfile для нового сервиса, оптимизируя его для работы в Kubernetes
3. **Проверка и отладка** : После развертывания нового сервиса в тестовом кластере я проводил проверку его работы, анализировал логи и исправлял возможные ошибки.
4. **Перенос в production** : После успешных тестов DevOps-инженер выполнял деплой нового сервиса в production-кластер.

----

# Фитнес-студия

(...рассказ про банк...) на первом проекте практически с нуля реализовывали бэк на fastapi, архитектура была готова (тим лид - так же devops был другом директора, и они развивали бизнес вместе) я расширил их команду на 3 бека. 

Это был сервис-моноли, который позволял взаимодействовать тренерам и пользователям друг с другом. У пользователей был лк, в котором они записывались к тренеру, отслеживали срок абонемента, отслеживали свой прогресс в тренировках, получали рассылки о новых направлениях и пробных занятиях.

### В моей ответственности было:
- написание моделей таблиц в sqlalchemy
- написал авторизацию/аутентификацию на jwt
- работа с celery - отправка email-уведомлений клиентов/тренеров/администратора 
- логика записи на тренировку, отмену занятий.

Брокер - redis, фоновые задачи - celery

### Команда:
1 project + тимлид
2 бэка
1 devops
1 фронт

### Процессы

Работали без процессов, были по необходимости рандомные встречи/обсуждения, ставились задачи на 1 неделю, канбан

### Факап

Пример факапа:

На начальном этапе при выборке пользователей с записями на тренировки я не использовал `joinedload` или `selectinload`, что привело к множеству лишних SQL-запросов и замедлению API.

**Как решил:**  
Обнаружил ошибку(тренер выгружал тренировки со своими клиентами и был длительный запрос), когда тестил с echo=True, далее, чтобы убедиться навесил explain и обнаружил проблему n+1, переписал запросы с selectinload 

**Чему научился:**  
Теперь всегда проверяю SQL-запросы через логи или инструменты вроде SQLAlchemy echo или pytest-sqlgold перед тем, как отправлять код в продакшен.

## Метрики

Собирали логи в Loki, а так же смотрели нагрузку через htop в консоли linux (нагрузки практически не было, до grafana не дожил проект)